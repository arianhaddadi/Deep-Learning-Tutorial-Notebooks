{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">\n",
    "    <img src=\"./files/Waterloo.gif\" width=\"20%\" height=\"20%\"> \n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <font size='6' color='blue'><b>Recurrent Neural Network</b></font><br/>\n",
    "    <font size='3' color='blue'><b>(Code Example)</b></font><br/>\n",
    "    <font size='3' color='blue'><b>Ali Ghodsi</b></font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Description:\n",
    "<p>We want to perform sentiment analysis on movie reviews from the Large Movie Review Dataset, sometimes known as the IMDB dataset. In this task, given a movie review, the model attempts to predict whether it is positive or negative. This is a binary classification task<sup><a href=\"#R1\">[1]</a></sup>.</p>\n",
    "\n",
    "### IMBD dataset in Keras:\n",
    "<p>Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".<sup><a href=\"#R2\">[2]</a></sup></p>\n",
    "(For more information visit reference <a href=\"#R2\" target=\"_blank\">[2]</a>.)\n",
    "<br /><br />\n",
    "<font color=\"red\"><b>Note:</b> Most parts of the fallowing code are captured from <a href=\"#R3\">[3]</a</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1)  Import packages and set parameters\n",
    "In the following code except LSTM layer, another new layer which is called embedding Layer is used. Later it will be explained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting paramiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences\n",
    "Pad the word sequences in each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3) Model Definition\n",
    "<b>Embedding layer:</b> Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]<sup><a href=\"#R4\" target=\"_blank\">[4]</a></sup> This layer can only be used as the first layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "#######################\n",
    "#  Embedding layer[4] #\n",
    "#######################\n",
    "# - the model will take as input an integer matrix of size (batch_size, input_length).\n",
    "# - the largest integer (i.e. word index) in the input should be no larger than (max_features-1) \n",
    "#   (vocabulary size).\n",
    "# - now model.output_shape == (None, input_length, 128), where None is the batch dimension.\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4) Compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5) Learning model and fit it on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 220s - loss: 0.4589 - acc: 0.7848 - val_loss: 0.3808 - val_acc: 0.8359\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 213s - loss: 0.2973 - acc: 0.8795 - val_loss: 0.4464 - val_acc: 0.8312\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 217s - loss: 0.2146 - acc: 0.9180 - val_loss: 0.4146 - val_acc: 0.8340\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 218s - loss: 0.1538 - acc: 0.9434 - val_loss: 0.4657 - val_acc: 0.8296\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 219s - loss: 0.1094 - acc: 0.9613 - val_loss: 0.5915 - val_acc: 0.8292\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 217s - loss: 0.0781 - acc: 0.9742 - val_loss: 0.6407 - val_acc: 0.8118\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 216s - loss: 0.0594 - acc: 0.9792 - val_loss: 0.7037 - val_acc: 0.8212\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 216s - loss: 0.0504 - acc: 0.9832 - val_loss: 0.7379 - val_acc: 0.8182\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 216s - loss: 0.0350 - acc: 0.9893 - val_loss: 0.8336 - val_acc: 0.8179\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 216s - loss: 0.0278 - acc: 0.9913 - val_loss: 0.8743 - val_acc: 0.8136\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 215s - loss: 0.0240 - acc: 0.9920 - val_loss: 0.9030 - val_acc: 0.8126\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 215s - loss: 0.0175 - acc: 0.9942 - val_loss: 1.0718 - val_acc: 0.8147\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 217s - loss: 0.0144 - acc: 0.9955 - val_loss: 1.0277 - val_acc: 0.8102\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 224s - loss: 0.0139 - acc: 0.9956 - val_loss: 1.1175 - val_acc: 0.8065\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 219s - loss: 0.0103 - acc: 0.9966 - val_loss: 1.1419 - val_acc: 0.8083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa0da039cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function saves the model in 'rnn.h5' file\n",
    "#Note: Uncomment it in the case of saving the trained model.\n",
    "\n",
    "# model.save('./rnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can load a pretrained model by using this function.\n",
    "model = keras.models.load_model('./rnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict labels of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "24992/25000 [============================>.] - ETA: 0s\n",
      "\n",
      "Test score: 1.14194533084\n",
      "Test accuracy: 0.80828\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating model...')\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('\\n\\nTest score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten predicted label and true label of test data\n",
      "[1 0 1 1 0 1 0 0 1 1]\n",
      "[1 1 1 1 1 0 0 0 1 1]\n",
      "\n",
      "Evaluating model (alternative way)...\n",
      "Test accuracy:  0.80828\n"
     ]
    }
   ],
   "source": [
    "y_prd = model.predict(x_test)\n",
    "\n",
    "y_prd = [1 if v > 0.5 else 0 for v in y_prd]\n",
    "print('First ten predicted label and true label of test data')\n",
    "print(np.array(y_prd[0:10]))\n",
    "print(y_test[0:10])\n",
    "\n",
    "# Alternative way for evaluating model\n",
    "print('\\nEvaluating model (alternative way)...')\n",
    "nb_correct_labels = np.sum(y_prd == y_test)\n",
    "print('Test accuracy: ', nb_correct_labels/len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<b id=\"R1\">[1]</b> http://deeplearning.net/tutorial/lstm.html <br />\n",
    "<b id=\"R2\">[2]</b> https://keras.io/datasets/<br />\n",
    "<b id=\"R3\">[3]</b> https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py<br />\n",
    "<b id=\"R4\">[4]</b> https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
