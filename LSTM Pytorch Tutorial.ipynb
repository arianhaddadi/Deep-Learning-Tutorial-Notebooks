{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8234ef01",
      "metadata": {
        "id": "8234ef01"
      },
      "source": [
        "# LSTM Example in Pytorch\n",
        "## STAT 940 - Deep Learning\n",
        "\n",
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ff35406",
      "metadata": {
        "id": "6ff35406"
      },
      "source": [
        "In this tutorial we will train an LSTM (Long short-term memory) Network for an NLP (Natural Language Processing) task in following 4 steps:\n",
        "- Step 1)  Import libraries and set parameters\n",
        "- Step 2) Load and prepare data \n",
        "- Step 3) Model set up\n",
        "- Step 4) Set up optimizer and loss, & train the model\n",
        "\n",
        "Also we will see how to:\n",
        "- Save and load learned models\n",
        "- Predict labels of test data\n",
        "- Check accuracy of model\n",
        "\n",
        "All hyperparameters are named in caps.\n",
        "\n",
        "**Disclaimer: This notebook is a demonstration of an LSTM model for NLP on PyTorch. Use it at your own risk.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b7037b7",
      "metadata": {
        "id": "7b7037b7"
      },
      "source": [
        "## STEP 1 - Import libraries and set up some configurations\n",
        "\n",
        "First, we will import the necessary libraries and packages that are needed for Convolutional Neural Network.\n",
        "- 1) Python essentials\n",
        "- 2) `torch`, `torch.nn`, `torch.nn.functional`, `torch.optim`: Pytorch imports for building custom neural networks containing\n",
        "    - Sequential model type: This provides linear stack of neural network layers\n",
        "    - Core layers (Linear, Dropout, ReLU): these layers are used in most neural networks\n",
        "    - Recurrent layers \n",
        "    - Data tools: `torch.utils.data.Dataset` for custom data sets, `torch.utils.data.DataLoader` for loading in data\n",
        "    - Optimizers in `torch.optim`\n",
        "- 3) `torchtext`: PyTorch package for NLP (Natural Language Processing) tasks, which contains popular datasets and utils for easy use.\n",
        "- 4) `tokenizers`: a [HuggingFace](https://huggingface.co/) library that enables quick tokenizer training and vocab building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "746d1fa2",
      "metadata": {
        "id": "746d1fa2"
      },
      "outputs": [],
      "source": [
        "# 1)\n",
        "import numpy as np\n",
        "# 2)\n",
        "import torch     \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# 3)\n",
        "#!pip install torchtext  # if your environment doesn't have this package \n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed everything!"
      ],
      "metadata": {
        "id": "WeAQqaF-Hx_q"
      },
      "id": "WeAQqaF-Hx_q"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6656a023",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6656a023",
        "outputId": "5c458dc4-f101-42ce-9da8-92bafda3258c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbe8e296bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "SEED = 940\n",
        "np.random.seed(SEED)    # set seed for reproducibility (within numpy)\n",
        "torch.manual_seed(SEED) # set seed for reproducibility (within pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set-up GPU (optional)\n",
        "\n",
        "Leverage GPUs for faster training of neural networks. This section will help you set up GPUs on PyTorch. Run this code whether or not you are using a GPU, and it will detect your current device and pass the info to PyTorch."
      ],
      "metadata": {
        "id": "DOlDTl5v4l9I"
      },
      "id": "DOlDTl5v4l9I"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check is GPU is available\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNqaV2xP4iab",
        "outputId": "208db8fe-0023-4a1c-d8f7-c929e7312ba6"
      },
      "id": "JNqaV2xP4iab",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c66de27",
      "metadata": {
        "id": "3c66de27"
      },
      "source": [
        "## STEP 2 - Load and prepare data\n",
        "\n",
        "Our toy dataset provided in the function `torchtext.datasets.IMDB()` is for a sentiment analysis task, in which we are required to classify movie reviews in English into two types of sentiments, positive or negative. This dataset was contributed by the AI research team at Stanford (see their [website](http://ai.stanford.edu/~amaas/data/sentiment/)).  It consists of 25,000 training examples and 25,000 test examples. After executing the below cell, you can check the `'./IMDB/aclImdb/README'` file for more info on the dataset.\n",
        "\n",
        "If the data is not stored at the PyTorch site, you might need to load data from your own directory using different types of dataset objects, or even writing a custom subclass of `torch.utils.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f0bbcddf",
      "metadata": {
        "id": "f0bbcddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a883648-2b0d-45fd-a96b-7c9355e083d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of training examples: 25000\n",
            "# of test examples: 25000\n"
          ]
        }
      ],
      "source": [
        "from torchtext.datasets import IMDB as imdb\n",
        "\n",
        "# Download dataset from pytorch and untar\n",
        "train = imdb(root='./', split='train')  # \"./\" represents the current working directory\n",
        "test = imdb(root='./', split='test')\n",
        "\n",
        "print('# of training examples:', len(train))\n",
        "print(\"# of test examples:\", len(test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show some examples\n",
        "train = list(train)\n",
        "test = list(test)\n",
        "train[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Gy28I6AZsQ",
        "outputId": "4ebd0521-8442-4612-a7bb-69b77428b55b"
      },
      "id": "i1Gy28I6AZsQ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('neg',\n",
              "  'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'),\n",
              " ('neg',\n",
              "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'),\n",
              " ('neg',\n",
              "  \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\"),\n",
              " ('neg',\n",
              "  \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\"),\n",
              " ('neg',\n",
              "  'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqq8giNuAeA7",
        "outputId": "926cb082-4d9d-45cb-9223-8bf2e4046272"
      },
      "id": "sqq8giNuAeA7",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('neg',\n",
              "  'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.'),\n",
              " ('neg',\n",
              "  \"Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\"),\n",
              " ('neg',\n",
              "  \"its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\"),\n",
              " ('neg',\n",
              "  \"STAR RATING: ***** Saturday Night **** Friday Night *** Friday Morning ** Sunday Night * Monday Morning <br /><br />Former New Orleans homicide cop Jack Robideaux (Jean Claude Van Damme) is re-assigned to Columbus, a small but violent town in Mexico to help the police there with their efforts to stop a major heroin smuggling operation into their town. The culprits turn out to be ex-military, lead by former commander Benjamin Meyers (Stephen Lord, otherwise known as Jase from East Enders) who is using a special method he learned in Afghanistan to fight off his opponents. But Jack has a more personal reason for taking him down, that draws the two men into an explosive final showdown where only one will walk away alive.<br /><br />After Until Death, Van Damme appeared to be on a high, showing he could make the best straight to video films in the action market. While that was a far more drama oriented film, with The Shepherd he has returned to the high-kicking, no brainer action that first made him famous and has sadly produced his worst film since Derailed. It's nowhere near as bad as that film, but what I said still stands.<br /><br />A dull, predictable film, with very little in the way of any exciting action. What little there is mainly consists of some limp fight scenes, trying to look cool and trendy with some cheap slo-mo/sped up effects added to them that sadly instead make them look more desperate. Being a Mexican set film, director Isaac Florentine has tried to give the film a Robert Rodriguez/Desperado sort of feel, but this only adds to the desperation.<br /><br />VD gives a particularly uninspired performance and given he's never been a Robert De Niro sort of actor, that can't be good. As the villain, Lord shouldn't expect to leave the beeb anytime soon. He gets little dialogue at the beginning as he struggles to muster an American accent but gets mysteriously better towards the end. All the supporting cast are equally bland, and do nothing to raise the films spirits at all.<br /><br />This is one shepherd that's strayed right from the flock. *\"),\n",
              " ('neg',\n",
              "  \"First off let me say, If you haven't enjoyed a Van Damme movie since bloodsport, you probably will not like this movie. Most of these movies may not have the best plots or best actors but I enjoy these kinds of movies for what they are. This movie is much better than any of the movies the other action guys (Segal and Dolph) have thought about putting out the past few years. Van Damme is good in the movie, the movie is only worth watching to Van Damme fans. It is not as good as Wake of Death (which i highly recommend to anyone of likes Van Damme) or In hell but, in my opinion it's worth watching. It has the same type of feel to it as Nowhere to Run. Good fun stuff!\")]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train[:]; del test[:]\n",
        "\n",
        "# delete all elements in the list to release memory\n",
        "# we won't be storing the datasets in RAM all the time"
      ],
      "metadata": {
        "id": "RWGJGSgcxQdB"
      },
      "id": "RWGJGSgcxQdB",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6b32f8b2",
      "metadata": {
        "id": "6b32f8b2"
      },
      "source": [
        "### Train tokenizer and build vocabulary\n",
        "\n",
        "This notebook shows you how to perform tokenization and vocab building using the HuggingFace library. Check out their [github repo](https://github.com/huggingface/tokenizers). \n",
        "\n",
        "To learn more about different trainable tokenizers & vocab builders, see this reddit [post](https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/). "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "7sCz__MwQHlm"
      },
      "id": "7sCz__MwQHlm",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "# get a tokenizer model\n",
        "tokenizer = Tokenizer(WordPiece())\n",
        "\n",
        "# pre-tokenizer splits a sentence to 'prototype' tokens according to a set of rules\n",
        "# which will then be fed to the tokenizer trainer \n",
        "# and the final trained tokens will be generated from disentangling and recombining those prototype tokens\n",
        "tokenizer.pre_tokenizer = Whitespace() \n",
        "\n",
        "def yield_text(data_iter):\n",
        "    '''Creates a generator for all texts in the whole dataset'''\n",
        "\n",
        "    for _, text in data_iter:\n",
        "        yield text\n",
        "\n",
        "VOCAB_SIZE = 30000\n",
        "trainer = WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\"])\n",
        "\n",
        "# the IMDB function returns an iterator of all datapoints\n",
        "# so you will see this line multiple times\n",
        "train_data = imdb(root='./', split='train')\n",
        "\n",
        "# train tokenizer on training data\n",
        "tokenizer.train_from_iterator(yield_text(train_data), trainer=trainer)"
      ],
      "metadata": {
        "id": "Rglz9z5EQT60"
      },
      "id": "Rglz9z5EQT60",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get tokens of a string\n",
        "tokenizer.encode('This is a notebook demonstrating the LSTM model for STAT 940.').tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KlNIQ317v0p",
        "outputId": "8d5fc55f-af26-4564-8eec-148dc191cc1d"
      },
      "id": "3KlNIQ317v0p",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'note',\n",
              " '##book',\n",
              " 'demonstrating',\n",
              " 'the',\n",
              " 'L',\n",
              " '##ST',\n",
              " '##M',\n",
              " 'model',\n",
              " 'for',\n",
              " 'ST',\n",
              " '##AT',\n",
              " '94',\n",
              " '##0',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get token ids of a string\n",
        "tokenizer.encode('This is a notebook demonstrating the LSTM model for STAT 940.').ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfnCVXDTSGBp",
        "outputId": "1101a346-1139-43a9-c151-1653c60c2873"
      },
      "id": "WfnCVXDTSGBp",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[524,\n",
              " 351,\n",
              " 67,\n",
              " 2924,\n",
              " 10474,\n",
              " 24657,\n",
              " 327,\n",
              " 46,\n",
              " 2131,\n",
              " 233,\n",
              " 4886,\n",
              " 388,\n",
              " 3407,\n",
              " 2059,\n",
              " 25702,\n",
              " 225,\n",
              " 16]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab is a dictionary that maps included tokens to integer ids\n",
        "# it is already computed when the tokenizer is trained, and stored in tokenizer.get_vocab()\n",
        "list(tokenizer.get_vocab().items())[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlGUeJpb8DYO",
        "outputId": "48241ffd-98d3-4f40-e632-e6fcd8afc06d"
      },
      "id": "YlGUeJpb8DYO",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('##irds', 6118),\n",
              " ('Cros', 12247),\n",
              " ('##OPLE', 16908),\n",
              " ('Transyl', 20237),\n",
              " ('EAR', 19142),\n",
              " ('stole', 9585),\n",
              " ('##arios', 12281),\n",
              " ('brainwashed', 22845),\n",
              " ('Firstly', 11266),\n",
              " ('town', 1786)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocab()[\"[UNK]\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8jFvhwz8-SF",
        "outputId": "62cf6cd7-360f-4a11-fb74-ec4280adbcfc"
      },
      "id": "F8jFvhwz8-SF",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define data pipelines\n",
        "\n",
        "Text pipeline: tokenize, map to integers, cut extra length\n",
        "\n",
        "Label pipeline: encode labels as integers"
      ],
      "metadata": {
        "id": "R2oeuYuTL1Bl"
      },
      "id": "R2oeuYuTL1Bl"
    },
    {
      "cell_type": "code",
      "source": [
        "MAXLEN = 80\n",
        "\n",
        "def cut(x, maxlen=128):\n",
        "    \"\"\"Cuts a sequence x if its length exceeds `maxlen` \"\"\"\n",
        "\n",
        "    return x[0:maxlen] if len(x) > maxlen else x\n",
        "\n",
        "def text_pipeline(x):\n",
        "    \"\"\"Pipeline of preprocessing text data. \n",
        "       Transformations include tokenizing, mapping from tokens to integers, and cut extra length\"\"\"\n",
        "\n",
        "    x = tokenizer.encode(x).ids\n",
        "    x = cut(x, maxlen=MAXLEN)\n",
        "    return x\n",
        "\n",
        "def label_pipeline(x): \n",
        "    \"\"\"Pipeline of encoding labels. \n",
        "    New label <- 1 if original label == 'pos', new label <- 0 if otherwise\"\"\"\n",
        "\n",
        "    return 1 if x == 'pos' else 0\n"
      ],
      "metadata": {
        "id": "xZA9sSWpwjKm"
      },
      "id": "xZA9sSWpwjKm",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step: **collate function for DataLoaders**.\n",
        "\n",
        "Pytorch `DataLoaders` are essential for loading data before training. Collate functions are a part of the algorithm inside the dataloaders that batch examples together from the training set, and convert the batched examples to a single tensor. \n",
        "\n",
        "However, in our case, since the sequences after the preprocessing pipeline still have varying lengths (some are shorter than `MAXLEN` and are left unprocessed in the pipeline), we need to zero pad those short ones to `MAXLEN` before batching them. Otherwise, PyTorch wouldn't stack tensors with different sizes together. This will be done inside the collate function `collate_batch` as well."
      ],
      "metadata": {
        "id": "cPDmWjg3zWWu"
      },
      "id": "cPDmWjg3zWWu"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Transforms texts and labels according to corresponding pipelines,\n",
        "    zero pad shorter sequences, and stack all sequences in the batch along a new axis\n",
        "    \n",
        "    Input: batch -- a list of tuples with size == BATCH_SIZE, \n",
        "            1st element of the tuple is the label, 2nd element of the tuple is the text\n",
        "    Output: a tuple of two tensors, the 1st tensor contains labels in the batch, of size (BATCH_SIZE,),\n",
        "            the 2nd tensor contains processed sequences in the batch, of size (BATCH_SIZE, MAXLEN)\"\"\"\n",
        "\n",
        "    label_list, text_list = [], []\n",
        "\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)  \n",
        "    # float type labels required for BCE loss\n",
        "    \n",
        "    # zero pad sequences that are shorter than MAXLEN == 128\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    \n",
        "    return label_list.to(device), text_list.to(device)\n",
        "\n",
        "# now you can call DataLoader to load up data\n",
        "# train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "#                              shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "xdRsuHvnw96A"
      },
      "id": "xdRsuHvnw96A",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8154d3e1",
      "metadata": {
        "id": "8154d3e1"
      },
      "source": [
        "## STEP 3 - Model set up\n",
        "Here, we define a model with an embedding layer, three bidirectional LSTM layers (`num_layers=3, bidir=True`), and a linear layer with sigmoid activation. This is for providing you with a flexibility to tweak your LSTM. For example, you can make it the most simple form by setting `bidir=False` so that it is one-directional again, and also setting `num_layers=1`. You can also pass in `dropout=0.5` to `nn.LSTM` to add dropout layers with probability 0.5 after every LSTM layer (except the final layer). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ef29fdc3",
      "metadata": {
        "id": "ef29fdc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093157da-3eb4-4387-de81-0ae02bbbf83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (embedding): Embedding(30000, 128, padding_idx=0)\n",
            "  (lstm): LSTM(128, 64, num_layers=3, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=384, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=3, \n",
        "                 output_size=1,\n",
        "                 bidir=True):\n",
        "        \"\"\"Defines individual layers and config the model class\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_size = output_size\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_dim,padding_idx=0)\n",
        "        self.num_layers = num_layers  # of recurrent layers in LSTM\n",
        "\n",
        "        # the embed vector at the padding_idx remains all zeros during training\n",
        "        self.lstm = nn.LSTM(embed_dim,hidden_dim,num_layers,\n",
        "                           batch_first=True,\n",
        "                           bidirectional=bidir)\n",
        "      \n",
        "        # A multiplier of layers if bidirectional\n",
        "        self.bidir = 2 if bidir else 1\n",
        "                \n",
        "        self.fc = nn.Linear(hidden_dim*self.num_layers*self.bidir, self.output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # weights have to be initialized away from zeros if using SGD\n",
        "        self.init_weights()   \n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initializes model parameters\"\"\"\n",
        "\n",
        "        initrange = 0.5\n",
        "        nn.init.uniform_(self.embedding.weight,-initrange,initrange)\n",
        "        for k in range(self.num_layers):\n",
        "            nn.init.uniform_(eval(\"self.lstm.weight_hh_l\"+str(k)), -initrange, initrange)\n",
        "            nn.init.uniform_(eval(\"self.lstm.weight_ih_l\"+str(k)), -initrange, initrange)\n",
        "            nn.init.zeros_(eval(\"self.lstm.bias_hh_l\"+str(k)))\n",
        "            nn.init.zeros_(eval(\"self.lstm.bias_ih_l\"+str(k)))\n",
        "        nn.init.uniform_(self.fc.weight,-initrange,initrange)\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        \"\"\"\n",
        "        Initializes hidden state and cell state. \n",
        "        The first output is the hidden state, and the 2nd is the cell state\"\"\"\n",
        "\n",
        "        return (torch.zeros(self.bidir*self.num_layers, bsz, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.bidir*self.num_layers, bsz, self.hidden_dim).to(device))\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        \"\"\"Foward pass function called during training\"\"\"\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded tensor: (BATCH_SIZE, MAXLEN, EMBED_DIM)\n",
        "\n",
        "        lstm_out, (hn, cn) = self.lstm(embedded, hidden)\n",
        "        # lstm_out: (BATCH_SIZE, MAXLEN, HIDDEN_DIM)\n",
        "\n",
        "        # swap 1st and 2nd axes so that datapoint indices corresponds to the 1st axis\n",
        "        hn = torch.transpose(hn,0,1)  \n",
        "\n",
        "        # for each datapoint, concat hidden states of all layers  \n",
        "        hn = hn.reshape(-1, self.num_layers*self.hidden_dim*self.bidir)\n",
        "        # hn: (BATCH_SIZE, NUM_LAYERS*bidir*HIDDEN_DIM)\n",
        "        \n",
        "        a = self.fc(hn)\n",
        "        # a: (BATCH_SIZE, 1)\n",
        "\n",
        "        return self.sigmoid(a).squeeze() # unroll dimensions with length 1, i.e. flattens output to 1d\n",
        "\n",
        "EMBED_DIM  = 128   # size of the embedding \n",
        "HIDDEN_DIM = 64    # size of the hidden layer in LSTM\n",
        "net = Net(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, num_layers=3, bidir=True).to(device)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1c4e75",
      "metadata": {
        "id": "4f1c4e75"
      },
      "source": [
        "## STEP 4 - Set up optimizer and loss, train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8a2d7f09",
      "metadata": {
        "id": "8a2d7f09"
      },
      "outputs": [],
      "source": [
        "LR = 1e-2                   # learning rate\n",
        "MOMENTUM = 0.9              # SGD momentum\n",
        "\n",
        "# binary cross entropy loss\n",
        "criterion = nn.BCELoss() \n",
        "\n",
        "# stochastic gradient descent as optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aa085c9",
      "metadata": {
        "id": "1aa085c9"
      },
      "source": [
        "#### Train & evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7bca6a88",
      "metadata": {
        "id": "7bca6a88"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    # tell the model that we're training it now\n",
        "    net.train()\n",
        "\n",
        "    # initialize hidden states to all zeros\n",
        "    hidden = net.init_hidden(bsz=BATCH_SIZE)\n",
        "\n",
        "    # initialize useful training metrics\n",
        "    total_acc, total_count, avg_loss = 0, 0, 0\n",
        "    log_interval = 100\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        # at the final batch, the actual batch size < BATCH_SIZE\n",
        "        # so reinit the hidden states to match the correct shapes\n",
        "        if label.size(0) != BATCH_SIZE:\n",
        "            hidden = net.init_hidden(bsz=label.size(0))\n",
        "\n",
        "        # reset gradients \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass \n",
        "        output = net(text, hidden) \n",
        "\n",
        "        # compute loss\n",
        "        loss = criterion(output, label)\n",
        "        avg_loss += loss/label.size(0)\n",
        "\n",
        "        # compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # count accurate predictions \n",
        "        total_acc += (torch.round(output).int() == label).sum().item()\n",
        "        \n",
        "        # count cumulative examples in the current epoch\n",
        "        total_count += label.size(0)\n",
        "\n",
        "        # report training progress every 100 batches\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| loss {:.4f} | accuracy {:8.3f}'.format(epoch, idx, len(dataloader), \n",
        "                                              avg_loss, total_acc/total_count))\n",
        "            # reset metrics every 100 batches\n",
        "            total_acc, total_count, avg_loss = 0, 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    # switch to evaluation mode\n",
        "    net.eval()\n",
        "    hidden = net.init_hidden(bsz=BATCH_SIZE)\n",
        "\n",
        "    total_acc, total_count, avg_loss = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            if label.size(0) != BATCH_SIZE:\n",
        "                hidden = net.init_hidden(bsz=label.size(0))\n",
        "            output = net(text, hidden)\n",
        "            loss = criterion(output, label)\n",
        "            avg_loss += loss/label.size(0)\n",
        "            total_acc += (torch.round(output).int() == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    \n",
        "    # return validation loss & accuracy\n",
        "    return avg_loss, total_acc/total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop"
      ],
      "metadata": {
        "id": "gXmGKTvgECph"
      },
      "id": "gXmGKTvgECph"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "EPOCHS = 40       # number of epochs\n",
        "BATCH_SIZE = 64   # batch size for training\n",
        "TRAIN_SPLIT = 0.8 # split 80% of training data to train model, 20% to end-of-epoch validation\n",
        "\n",
        "train_data = imdb(root='./', split='train')\n",
        "\n",
        "# in order to shuffle data during training (which is essential for training)\n",
        "# convert the dataset type to map-style\n",
        "train_data = to_map_style_dataset(train_data)\n",
        "\n",
        "# training and validation split\n",
        "num_train = int(len(train_data) * TRAIN_SPLIT)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_data, [num_train, len(train_data) - num_train])\n",
        "\n",
        "# data loaders for the training loop\n",
        "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # record starting time of the epoch\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # training\n",
        "    train(train_loader)\n",
        "\n",
        "    # validation\n",
        "    loss_val, accu_val = evaluate(valid_loader)\n",
        "\n",
        "    print('-' * 90)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s '\n",
        "          '| valid loss {:.4f} | valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           loss_val, accu_val))\n",
        "    print('-' * 90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlInGKGbEEDl",
        "outputId": "00f9fc52-bc2a-46c0-e722-cf553869f7d4"
      },
      "id": "BlInGKGbEEDl",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/  313 batches | loss 1.1711 | accuracy    0.504\n",
            "| epoch   1 |   200/  313 batches | loss 1.1177 | accuracy    0.518\n",
            "| epoch   1 |   300/  313 batches | loss 1.1058 | accuracy    0.514\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 14.69s | | valid loss 0.9581 | valid accuracy    0.522 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/  313 batches | loss 1.0859 | accuracy    0.553\n",
            "| epoch   2 |   200/  313 batches | loss 1.0768 | accuracy    0.547\n",
            "| epoch   2 |   300/  313 batches | loss 1.0741 | accuracy    0.546\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 14.35s | | valid loss 0.9450 | valid accuracy    0.536 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  313 batches | loss 1.0702 | accuracy    0.573\n",
            "| epoch   3 |   200/  313 batches | loss 1.0553 | accuracy    0.577\n",
            "| epoch   3 |   300/  313 batches | loss 1.0480 | accuracy    0.583\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 14.19s | | valid loss 0.9368 | valid accuracy    0.547 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  313 batches | loss 1.0458 | accuracy    0.602\n",
            "| epoch   4 |   200/  313 batches | loss 1.0349 | accuracy    0.602\n",
            "| epoch   4 |   300/  313 batches | loss 1.0409 | accuracy    0.596\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 14.35s | | valid loss 0.9321 | valid accuracy    0.546 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  313 batches | loss 1.0243 | accuracy    0.622\n",
            "| epoch   5 |   200/  313 batches | loss 1.0186 | accuracy    0.619\n",
            "| epoch   5 |   300/  313 batches | loss 1.0214 | accuracy    0.610\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 14.25s | | valid loss 0.9403 | valid accuracy    0.552 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  313 batches | loss 1.0015 | accuracy    0.641\n",
            "| epoch   6 |   200/  313 batches | loss 0.9921 | accuracy    0.644\n",
            "| epoch   6 |   300/  313 batches | loss 0.9968 | accuracy    0.634\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 14.23s | | valid loss 0.9294 | valid accuracy    0.573 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  313 batches | loss 0.9650 | accuracy    0.669\n",
            "| epoch   7 |   200/  313 batches | loss 0.9663 | accuracy    0.657\n",
            "| epoch   7 |   300/  313 batches | loss 0.9717 | accuracy    0.652\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 14.18s | | valid loss 0.9835 | valid accuracy    0.573 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  313 batches | loss 0.9351 | accuracy    0.680\n",
            "| epoch   8 |   200/  313 batches | loss 0.9203 | accuracy    0.687\n",
            "| epoch   8 |   300/  313 batches | loss 0.9363 | accuracy    0.677\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 14.16s | | valid loss 0.9170 | valid accuracy    0.596 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  313 batches | loss 0.8883 | accuracy    0.717\n",
            "| epoch   9 |   200/  313 batches | loss 0.8890 | accuracy    0.706\n",
            "| epoch   9 |   300/  313 batches | loss 0.8850 | accuracy    0.704\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 14.25s | | valid loss 0.9867 | valid accuracy    0.579 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  313 batches | loss 0.8640 | accuracy    0.723\n",
            "| epoch  10 |   200/  313 batches | loss 0.8423 | accuracy    0.728\n",
            "| epoch  10 |   300/  313 batches | loss 0.8416 | accuracy    0.726\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 14.66s | | valid loss 1.0057 | valid accuracy    0.575 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  11 |   100/  313 batches | loss 0.8034 | accuracy    0.750\n",
            "| epoch  11 |   200/  313 batches | loss 0.8011 | accuracy    0.748\n",
            "| epoch  11 |   300/  313 batches | loss 0.8014 | accuracy    0.743\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 14.39s | | valid loss 0.9563 | valid accuracy    0.623 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  12 |   100/  313 batches | loss 0.7426 | accuracy    0.780\n",
            "| epoch  12 |   200/  313 batches | loss 0.7432 | accuracy    0.777\n",
            "| epoch  12 |   300/  313 batches | loss 0.7790 | accuracy    0.754\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 14.31s | | valid loss 0.9866 | valid accuracy    0.611 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  13 |   100/  313 batches | loss 0.7143 | accuracy    0.788\n",
            "| epoch  13 |   200/  313 batches | loss 0.6992 | accuracy    0.791\n",
            "| epoch  13 |   300/  313 batches | loss 0.7100 | accuracy    0.789\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 14.16s | | valid loss 1.0174 | valid accuracy    0.624 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  14 |   100/  313 batches | loss 0.6422 | accuracy    0.820\n",
            "| epoch  14 |   200/  313 batches | loss 0.6407 | accuracy    0.806\n",
            "| epoch  14 |   300/  313 batches | loss 0.6709 | accuracy    0.801\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 14.20s | | valid loss 1.0484 | valid accuracy    0.602 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  15 |   100/  313 batches | loss 0.5493 | accuracy    0.856\n",
            "| epoch  15 |   200/  313 batches | loss 0.5898 | accuracy    0.833\n",
            "| epoch  15 |   300/  313 batches | loss 0.6586 | accuracy    0.805\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 14.22s | | valid loss 1.0879 | valid accuracy    0.612 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  16 |   100/  313 batches | loss 0.4886 | accuracy    0.875\n",
            "| epoch  16 |   200/  313 batches | loss 0.5194 | accuracy    0.858\n",
            "| epoch  16 |   300/  313 batches | loss 0.5349 | accuracy    0.853\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 14.16s | | valid loss 1.2200 | valid accuracy    0.624 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  17 |   100/  313 batches | loss 0.4495 | accuracy    0.885\n",
            "| epoch  17 |   200/  313 batches | loss 0.5100 | accuracy    0.853\n",
            "| epoch  17 |   300/  313 batches | loss 0.4681 | accuracy    0.875\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 14.23s | | valid loss 1.2061 | valid accuracy    0.619 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  18 |   100/  313 batches | loss 0.3724 | accuracy    0.909\n",
            "| epoch  18 |   200/  313 batches | loss 0.4216 | accuracy    0.887\n",
            "| epoch  18 |   300/  313 batches | loss 0.4364 | accuracy    0.879\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 14.17s | | valid loss 1.2927 | valid accuracy    0.618 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  19 |   100/  313 batches | loss 0.3370 | accuracy    0.919\n",
            "| epoch  19 |   200/  313 batches | loss 0.3504 | accuracy    0.913\n",
            "| epoch  19 |   300/  313 batches | loss 0.3945 | accuracy    0.889\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 14.24s | | valid loss 1.3150 | valid accuracy    0.612 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  20 |   100/  313 batches | loss 0.2997 | accuracy    0.928\n",
            "| epoch  20 |   200/  313 batches | loss 0.2651 | accuracy    0.938\n",
            "| epoch  20 |   300/  313 batches | loss 0.3105 | accuracy    0.924\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 14.19s | | valid loss 1.5266 | valid accuracy    0.613 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  21 |   100/  313 batches | loss 0.2212 | accuracy    0.954\n",
            "| epoch  21 |   200/  313 batches | loss 0.2452 | accuracy    0.943\n",
            "| epoch  21 |   300/  313 batches | loss 0.3024 | accuracy    0.924\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 14.11s | | valid loss 1.6168 | valid accuracy    0.622 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  22 |   100/  313 batches | loss 0.2023 | accuracy    0.959\n",
            "| epoch  22 |   200/  313 batches | loss 0.2106 | accuracy    0.955\n",
            "| epoch  22 |   300/  313 batches | loss 0.2384 | accuracy    0.944\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 13.97s | | valid loss 1.5778 | valid accuracy    0.628 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  23 |   100/  313 batches | loss 0.1978 | accuracy    0.958\n",
            "| epoch  23 |   200/  313 batches | loss 0.2008 | accuracy    0.953\n",
            "| epoch  23 |   300/  313 batches | loss 0.2189 | accuracy    0.948\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 13.89s | | valid loss 1.7624 | valid accuracy    0.614 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  24 |   100/  313 batches | loss 0.1321 | accuracy    0.974\n",
            "| epoch  24 |   200/  313 batches | loss 0.1518 | accuracy    0.967\n",
            "| epoch  24 |   300/  313 batches | loss 0.1642 | accuracy    0.962\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 13.97s | | valid loss 1.8064 | valid accuracy    0.627 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  25 |   100/  313 batches | loss 0.0911 | accuracy    0.985\n",
            "| epoch  25 |   200/  313 batches | loss 0.1071 | accuracy    0.977\n",
            "| epoch  25 |   300/  313 batches | loss 0.1063 | accuracy    0.979\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 13.84s | | valid loss 1.8990 | valid accuracy    0.614 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  26 |   100/  313 batches | loss 0.0614 | accuracy    0.991\n",
            "| epoch  26 |   200/  313 batches | loss 0.0665 | accuracy    0.989\n",
            "| epoch  26 |   300/  313 batches | loss 0.1016 | accuracy    0.979\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 14.19s | | valid loss 2.1680 | valid accuracy    0.626 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  27 |   100/  313 batches | loss 0.1080 | accuracy    0.978\n",
            "| epoch  27 |   200/  313 batches | loss 0.1034 | accuracy    0.978\n",
            "| epoch  27 |   300/  313 batches | loss 0.2163 | accuracy    0.949\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 13.96s | | valid loss 2.0632 | valid accuracy    0.620 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  28 |   100/  313 batches | loss 0.1441 | accuracy    0.970\n",
            "| epoch  28 |   200/  313 batches | loss 0.1549 | accuracy    0.965\n",
            "| epoch  28 |   300/  313 batches | loss 0.1316 | accuracy    0.968\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 14.09s | | valid loss 2.0578 | valid accuracy    0.624 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  29 |   100/  313 batches | loss 0.0942 | accuracy    0.981\n",
            "| epoch  29 |   200/  313 batches | loss 0.1086 | accuracy    0.975\n",
            "| epoch  29 |   300/  313 batches | loss 0.1225 | accuracy    0.973\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 14.41s | | valid loss 1.9307 | valid accuracy    0.628 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  30 |   100/  313 batches | loss 0.0827 | accuracy    0.984\n",
            "| epoch  30 |   200/  313 batches | loss 0.0541 | accuracy    0.991\n",
            "| epoch  30 |   300/  313 batches | loss 0.0721 | accuracy    0.986\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 14.35s | | valid loss 2.1900 | valid accuracy    0.633 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  31 |   100/  313 batches | loss 0.1047 | accuracy    0.977\n",
            "| epoch  31 |   200/  313 batches | loss 0.0823 | accuracy    0.984\n",
            "| epoch  31 |   300/  313 batches | loss 0.0689 | accuracy    0.988\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 14.31s | | valid loss 2.1814 | valid accuracy    0.624 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  32 |   100/  313 batches | loss 0.1225 | accuracy    0.972\n",
            "| epoch  32 |   200/  313 batches | loss 0.0884 | accuracy    0.983\n",
            "| epoch  32 |   300/  313 batches | loss 0.0671 | accuracy    0.988\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 14.26s | | valid loss 2.2769 | valid accuracy    0.628 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  33 |   100/  313 batches | loss 0.0471 | accuracy    0.992\n",
            "| epoch  33 |   200/  313 batches | loss 0.0336 | accuracy    0.995\n",
            "| epoch  33 |   300/  313 batches | loss 0.0275 | accuracy    0.996\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 14.45s | | valid loss 2.3594 | valid accuracy    0.631 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  34 |   100/  313 batches | loss 0.0218 | accuracy    0.997\n",
            "| epoch  34 |   200/  313 batches | loss 0.0173 | accuracy    0.997\n",
            "| epoch  34 |   300/  313 batches | loss 0.0223 | accuracy    0.997\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 14.32s | | valid loss 2.5996 | valid accuracy    0.628 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  35 |   100/  313 batches | loss 0.0125 | accuracy    0.999\n",
            "| epoch  35 |   200/  313 batches | loss 0.0151 | accuracy    0.998\n",
            "| epoch  35 |   300/  313 batches | loss 0.0198 | accuracy    0.997\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 14.23s | | valid loss 2.5989 | valid accuracy    0.634 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  36 |   100/  313 batches | loss 0.0431 | accuracy    0.992\n",
            "| epoch  36 |   200/  313 batches | loss 0.0263 | accuracy    0.996\n",
            "| epoch  36 |   300/  313 batches | loss 0.0189 | accuracy    0.997\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 14.24s | | valid loss 2.5390 | valid accuracy    0.631 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  37 |   100/  313 batches | loss 0.0229 | accuracy    0.997\n",
            "| epoch  37 |   200/  313 batches | loss 0.0143 | accuracy    0.998\n",
            "| epoch  37 |   300/  313 batches | loss 0.0259 | accuracy    0.997\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 14.22s | | valid loss 2.5918 | valid accuracy    0.619 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  38 |   100/  313 batches | loss 0.0206 | accuracy    0.997\n",
            "| epoch  38 |   200/  313 batches | loss 0.0194 | accuracy    0.997\n",
            "| epoch  38 |   300/  313 batches | loss 0.0255 | accuracy    0.995\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 14.22s | | valid loss 2.6129 | valid accuracy    0.627 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  39 |   100/  313 batches | loss 0.0552 | accuracy    0.988\n",
            "| epoch  39 |   200/  313 batches | loss 0.0440 | accuracy    0.991\n",
            "| epoch  39 |   300/  313 batches | loss 0.0921 | accuracy    0.978\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 14.17s | | valid loss 2.4224 | valid accuracy    0.620 \n",
            "------------------------------------------------------------------------------------------\n",
            "| epoch  40 |   100/  313 batches | loss 0.0707 | accuracy    0.987\n",
            "| epoch  40 |   200/  313 batches | loss 0.0466 | accuracy    0.991\n",
            "| epoch  40 |   300/  313 batches | loss 0.0811 | accuracy    0.981\n",
            "------------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 14.18s | | valid loss 2.6489 | valid accuracy    0.637 \n",
            "------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load the model"
      ],
      "metadata": {
        "id": "tah81zOu_88p"
      },
      "id": "tah81zOu_88p"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b4a380ad",
      "metadata": {
        "id": "b4a380ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373d1895-a5c1-4fbe-f61b-ab327472e1d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model_path = './imdb_lstm.pth'\n",
        "\n",
        "torch.save(net.state_dict(), model_path) # save model to path\n",
        "\n",
        "net = Net(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(device)\n",
        "net.load_state_dict(torch.load(model_path)) # load the weights of saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check test accuracy of model"
      ],
      "metadata": {
        "id": "gwmiVW9SACZt"
      },
      "id": "gwmiVW9SACZt"
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = imdb(root='./', split='test')\n",
        "test_data = to_map_style_dataset(test_data)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             collate_fn=collate_batch) \n",
        "# if your test data is unlabelled you might need to manually include the text_pipeline inside the training function, \n",
        "# as our collate_batch calls the label_pipeline, which is not useful for unlabelled data\n",
        "# so in that case you won't pass in collate_fn = collate_batch\n",
        "\n",
        "# But here we do have labels in the test set so there's no problem\n",
        "\n",
        "_, test_acc = evaluate(test_loader)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "L17UpSjHAKPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a12a9f8-3aa5-4057-fd4e-43723dcbfe37"
      },
      "id": "L17UpSjHAKPJ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.62888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get predictions from one batch"
      ],
      "metadata": {
        "id": "WGq78JQ6QLDr"
      },
      "id": "WGq78JQ6QLDr"
    },
    {
      "cell_type": "code",
      "source": [
        "_, seqs = next(iter(test_loader))\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    h = net.init_hidden(bsz=seqs.size(0))\n",
        "    outputs = net(seqs, h)\n",
        "    preds = torch.round(outputs).int()\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41GZ39QGQNRP",
        "outputId": "ffddf89c-a08d-48d4-8582-5502836d59df"
      },
      "id": "41GZ39QGQNRP",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
              "        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
              "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0',\n",
              "       dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "LSTM_pytorch_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}